{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4875d80d-154b-4729-b9bb-226a0905eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from datasets import load_from_disk\n",
    "import mlx.core as mx\n",
    "from mlx_vlm import load, generate\n",
    "from mlx_vlm.prompt_utils import apply_chat_template\n",
    "from mlx_vlm.utils import load_config\n",
    "from evaluaion_utils import evaluate_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edba8ea3-a54a-4ed0-8f9b-728c9284894e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlx/lib/python3.11/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564b5b92b0b4461e9252a7f3d8994268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b95824dca146688eca656963932bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"mlx-community/Qwen2.5-VL-3B-Instruct-bf16\"\n",
    "model, processor = load(model_path)\n",
    "config = load_config(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de742568-82cf-45d0-8865-8210ccd603f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an expert Visual Question Answering (VQA) system specializing in analyzing financial documents. Your task is to answer a question by generating a JSON object with the precise answer, its components, and its location.\n",
    "\n",
    "**Output Rules:**\n",
    "1. Your response MUST be a single, raw JSON object. Do not include explanations or markdown formatting.\n",
    "2. Analyze the question and the image to determine the type of answer required, then generate a JSON object using **only the keys appropriate for that answer type**, as defined below.\n",
    "3. All bounding box coordinates must be in `[x1, y1, x2, y2]`.\n",
    "4. All numeric answers must be returned as JSON numbers (floats or integers), not strings.\n",
    "\n",
    "**JSON Output Structures:**\n",
    "\n",
    "* **1. For Simple Extractive Answers**:\n",
    "    * `\"answer\"`: (float or string) The value found in the single cell.\n",
    "    * `\"answer_bbox\"`: (list[float]) The bounding box of the answer cell.\n",
    "\n",
    "* **2. For Answers Requiring Calculation/Aggregation**:\n",
    "    * `\"answer\"`: (float) The final **calculated** numeric value.\n",
    "    * `\"individual_answers\"`: (list[float]) The list of component numeric values used in the calculation.\n",
    "    * `\"individual_answers_bboxes\"`: (list[list[float]]) The bounding boxes for each component value.\n",
    "\n",
    "* **3. For Key-Value Identification**:\n",
    "    * `\"answer\"`: (float) The final value part of the pair that answers the question.\n",
    "    * `\"answer_key\"`: (string) The key part of the pair (e.g., the row header).\n",
    "    * `\"answer_key_bbox\"`: (list[float]) The bounding box for the `answer_key` text.\n",
    "    * `\"individual_answers\"`: (list[float]) The list of all numeric values considered to find the answer.\n",
    "    * `\"individual_answers_bboxes\"`: (list[list[float]]) The bounding boxes for each value in `individual_answers`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48a7f27f-2b8b-4d45-b613-4526ab482544",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "dataset = load_from_disk('EviFiVQA_val_dataset')\n",
    "\n",
    "\n",
    "if n > len(dataset):\n",
    "    print(f\"Warning: Requested {n} samples, but dataset only has {len(dataset)} samples.\")\n",
    "    n = len(dataset)\n",
    "\n",
    "sample_indices = random.sample(range(len(dataset)), n)\n",
    "samples = dataset.select(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14692595-c798-47f7-974f-54f73bf6d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_bounding_boxes(gt_dict: dict, width: int, height: int) -> dict:\n",
    "    \"\"\"\n",
    "    Recursively finds and scales all normalized bounding boxes in the ground truth\n",
    "    dictionary to absolute pixel coordinates and converts them to integers.\n",
    "\n",
    "    Args:\n",
    "        gt_dict (dict): The ground truth dictionary with normalized bboxes.\n",
    "        width (int): The original width of the image.\n",
    "        height (int): The original height of the image.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary with bboxes scaled to image dimensions.\n",
    "    \"\"\"\n",
    "    # Create a deep copy to avoid modifying the original data\n",
    "    scaled_gt = json.loads(json.dumps(gt_dict))\n",
    "\n",
    "    for key, value in scaled_gt.items():\n",
    "        if not value:\n",
    "            continue\n",
    "        # Scale a single bounding box\n",
    "        if key.endswith('_bbox'):\n",
    "            scaled_gt[key] = [\n",
    "                int(value[0] * width),\n",
    "                int(value[1] * height),\n",
    "                int(value[2] * width),\n",
    "                int(value[3] * height)\n",
    "            ]\n",
    "        # Scale a list of bounding boxes\n",
    "        elif key.endswith('_bboxes'):\n",
    "            scaled_boxes = []\n",
    "            for box in value:\n",
    "                scaled_boxes.append([\n",
    "                    int(box[0] * width),\n",
    "                    int(box[1] * height),\n",
    "                    int(box[2] * width),\n",
    "                    int(box[3] * height)\n",
    "                ])\n",
    "            scaled_gt[key] = scaled_boxes\n",
    "    return scaled_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c793140-264f-4009-8b9f-9c77637f64a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing 10 random samples from dataset ---\n",
      "\n",
      "--- Sample 1/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 100,\n",
      "  \"answer_key\": \"Cash and cash equivalents at end of year\",\n",
      "  \"answer_key_bbox\": [1000, 2200, 1050, 2220],\n",
      "  \"individual_answers\": [100, 2200, 1050, 2220],\n",
      "  \"individual_answers_bboxes\": [[1000, 2200, 1050, 2220]]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 0.058808509943736174, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 2/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 209,\n",
      "  \"answer_bbox\": [100, 270, 120, 280]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 1.0, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 3/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 1505,\n",
      "  \"answer_bbox\": [1000, 100, 1050, 110]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 0.4249182927993987, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 4/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 188,\n",
      "  \"answer_key\": \"Mobile Platform Fees under ASC 606\",\n",
      "  \"answer_key_bbox\": [1000, 100, 1120, 110],\n",
      "  \"individual_answers\": [3357, 2564, 793, 530, 188, 75],\n",
      "  \"individual_answers_bboxes\": [[1000, 100, 1120, 110], [1120, 100, 1240, 110], [1240, 100, 1360, 110], [1360, 100, 1480, 110], [1480, 100, 1600, 110], [1600, 100, 1720, 110]]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 1.0, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 5/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 118878,\n",
      "  \"answer_key\": \"European Revolving Loan Facility\",\n",
      "  \"answer_key_bbox\": [18, 318, 430, 352],\n",
      "  \"individual_answers\": [118878],\n",
      "  \"individual_answers_bboxes\": [[18, 318, 430, 352]]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 1.0, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 6/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 294,\n",
      "  \"answer_bbox\": [405, 160, 430, 175]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 1.0, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 7/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 557,\n",
      "  \"answer_bbox\": [100, 280, 190, 300]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 1.0, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 8/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 0.5,\n",
      "  \"answer_bbox\": [500, 140, 520, 150]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 0.3333333333333333, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 9/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 1.0,\n",
      "  \"individual_answers\": [117, 41, 19],\n",
      "  \"individual_answers_bboxes\": [[100, 200, 150, 210], [100, 220, 150, 230], [100, 240, 150, 250]]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 0.2388874260765363, 'TotalScore': 0.0}\n",
      "\n",
      "--- Sample 10/10 ---\n",
      "  Generating response...\n",
      "  Model Raw Output:\n",
      "```json\n",
      "{\n",
      "  \"answer\": 73,\n",
      "  \"answer_key\": \"Other\",\n",
      "  \"answer_key_bbox\": [100, 200, 120, 210],\n",
      "  \"individual_answers\": [75, 73],\n",
      "  \"individual_answers_bboxes\": [[100, 200, 120, 210], [100, 210, 120, 220]]\n",
      "}\n",
      "```\n",
      "  Scores: {'EvidenceF1': 0.0, 'AnswerScore_avg': 1.0, 'TotalScore': 0.0}\n"
     ]
    }
   ],
   "source": [
    "all_scores = []\n",
    "print(f\"\\n--- Processing {n} random samples from dataset ---\")\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f\"\\n--- Sample {i+1}/{n} ---\")\n",
    "    question = sample['question']\n",
    "    image_path = sample['image']\n",
    "    ground_truth_str = sample['ground_truth']\n",
    "    image = [Image.open(image_path).convert(\"RGB\")]\n",
    "    width, height = image[0].size\n",
    "    user_prompt = f\"Based on the provided image and question, generate the detailed JSON answer.\\n\\n## Question:\\n{question}\\n\\n## Image:\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": user_prompt}]}\n",
    "    ]\n",
    "    prompt  = apply_chat_template(processor, config, messages, num_images=len(image))\n",
    "    \n",
    "    print(\"  Generating response...\")\n",
    "    response = generate(model, processor, prompt, image, verbose=False, max_tokens=2048)\n",
    "    prediction_str = response[0]\n",
    "    print(f\"  Model Raw Output:\\n{prediction_str}\")\n",
    "\n",
    "    try:\n",
    "        json_part = prediction_str[prediction_str.find('{'):prediction_str.rfind('}')+1]\n",
    "        prediction_dict = json.loads(json_part)\n",
    "        ground_truth_dict = json.loads(ground_truth_str)\n",
    "        scaled_ground_truth_dict = scale_bounding_boxes(ground_truth_dict, width, height)\n",
    "        # Use the imported evaluation function\n",
    "        scores = evaluate_sample(prediction_dict, scaled_ground_truth_dict)\n",
    "        all_scores.append(scores)\n",
    "        print(f\"  Scores: {scores}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"  ERROR: Failed to decode JSON from model output. Assigning score of 0.\")\n",
    "        all_scores.append({\"EvidenceF1\": 0.0, \"AnswerScore_avg\": 0.0, \"TotalScore\": 0.0})\n",
    "    except Exception as e:\n",
    "        print(f\"  An unexpected error occurred during evaluation: {e}\")\n",
    "        all_scores.append({\"EvidenceF1\": 0.0, \"AnswerScore_avg\": 0.0, \"TotalScore\": 0.0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b55401-09ba-4277-b628-45652fd2d884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================\n",
      "--- Final Average Scores ---\n",
      "Average Evidence F1:    0.0000\n",
      "Average Answer Score:   0.7056\n",
      "Average Total Score:    0.0000\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "# 5. Final Summary\n",
    "if all_scores:\n",
    "    avg_evidence = np.mean([s['EvidenceF1'] for s in all_scores])\n",
    "    avg_answer = np.mean([s['AnswerScore_avg'] for s in all_scores])\n",
    "    avg_total = np.mean([s['TotalScore'] for s in all_scores])\n",
    "    print(\"\\n\" + \"=\"*45); print(\"--- Final Average Scores ---\")\n",
    "    print(f\"Average Evidence F1:    {avg_evidence:.4f}\")\n",
    "    print(f\"Average Answer Score:   {avg_answer:.4f}\")\n",
    "    print(f\"Average Total Score:    {avg_total:.4f}\")\n",
    "    print(\"=\"*45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
